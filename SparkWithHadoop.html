<!doctype html>
<html style='font-size:13px !important'>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>SparkWithHadoop</title><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
mark .md-meta { color: rgb(0, 0, 0); opacity: 0.3 !important; }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}


/* ============================================
 * ============ TYPORA lATEX THEME ============
 * ============================================
 */

/* userCustom on Windows */

:root {

/* == 字体设置 == */

	/* 基准字体 */
	--base-Latin-font: "Latin Modern Roman", "Latin Modern Roman 10"; /* 备选：Times, "Times New Roman" */
	--base-Chinese-font: 宋体-简, 华文宋体;
	--base-font-size: 9.5pt;

	/* 引言字体 */
	--quote-font: "Latin Modern Roman", "Latin Modern Roman 10", Times, "Times New Roman", 华文仿宋;
	--quote-font-size: 10pt; /* 这里字体用了10pt,比正文字体稍大 */

	/* 代码字体（代码中的中文会调用ui-font） */
	--code-font: "Latin Modern Mono", "Latin Modern Mono 10";

	/* 侧边栏字体 */
	--ui-font: "阿里巴巴普惠体 2.0";

	/* source mode 字体 */
	--sourceMode-font: "SF Mono", "阿里巴巴普惠体 2.0"; /* 默认调用code-font和ui-font */

	/* 目录字体 */
	--toc-font: ""; /* 默认调用base-font */
	--toc-font-size: ""; /* 默认调用base-font-size */

	/* 表格字体 */
	--table-title-font: ""; /* 默认调用heading-font */
	--table-font: ""; /* 默认调用base-font */

	/* 标题字体（总设置） */
	--heading-Latin-font: var(--base-Latin-font); /* 默认调用base-font(bold) */
	--heading-Chinese-font: 华文黑体;

	/* 标题字体分别设置 */
		/* 大标题（h1）字体 */
		--title-Chinese-font: "华文黑体"; 
		--title-font-size: 18pt;
		/* h2字体 */
		--h2-Chinese-font: 华文黑体; 
		--h2-font-size: 14pt;
		/* h3字体 */
		--h3-Chinese-font: "华文黑体";
		--h3-font-size: 12pt;
		/* h4字体 */
		--h4-Chinese-font: 华文楷体;
		--h4-font-size: 10pt;
		/* h5字体 */
		--h5-Chinese-font: 华文仿宋;
		--h5-font-size: 10pt;
		/* h6字体 */
		--h6-Chinese-font: 华文仿宋;
		--h6-font-size: 9.5pt;

	/* 粗体样式设置 */
	--strong-weight: 900; /* 加粗风格时使用的字重；400等同于normal，700等同于bold，900等同于heavy */ 

	/* 基础行距 */
	--base-line-height: 16pt; 

/* == 页面设置 == */
	/* 打印页边距 */
	--set-margin: 1.8cm 2cm 1.2cm 2cm !important;
	/* 参阅 <https://github.com/Keldos-Li/typora-latex-theme/blob/main/README.md#特别注意> */

/* == 控制设置 == */
	/* 目录中是否显示一级标题 */
	--toc-show-title: none; 
}
/* ============================================
 * ============ TYPORA lATEX THEME ============
 * ============================================
 * THIS PROJECT IS BASED ON THE WORKS OF YFZHAO20 AND DU33169, 
   WHOSE REPOSITORIES ARE ADRESSED ON 
   <https://github.com/yfzhao20/Typora-markdown>
   AND <https://github.com/du33169/typora-theme-essay_cn>,
   MANY THANKS TO THEM.
 * "TYPORA LATEX THEME" (MARKDOWN LATEX THEME)
   IS A THEME FOR TYPORA (OR CSS FOR OTHER MARKDOWN EDITORS),
   DESIGNED FOR CHINESE UNIVERSITY STUDENTS 
   USED IN LATEX-STYLED ESSAYS OR EXPERIMENTAL REPORTS.
 * COPYRIGHT (C) 2021 KELDOS; 
   REPOSITORY ADDRESS: <https://github.com/Keldos-Li/typora-latex-theme>;
   LISENCED UNDER GPL v3.0, SEE <https://github.com/Keldos-Li/typora-latex-theme/blob/main/LICENSE>
 */

/* LaTeX-light on Windows */

@import "";

body {
    padding: 0 !important;
    margin: 0 !important;
    line-height: var(--base-line-height);
    /*counter-reset: tableHead 0 imgHead 0;*/
}

/* 正文区基本属性 */
#write {
    font-family: var(--base-Latin-font), var(--base-Chinese-font), serif;
    font-size: var(--base-font-size);
    max-width: 21cm;/* A4标准宽度 */
    background-color: white;
    /* column-count: 2;
    column-gap: 25px;
    column-width: 8cm; 
    display: inline-block; */
    /* 这里可以试分栏的，但确实不适合实现 */
}

strong {
    font-weight: var(--strong-weight);
}

@media screen {
    #write {
        padding: var(--set-margin);
        /*border: 0.8px solid #AAC ; /* 添加一个淡蓝色的边框 */
        box-shadow: 0 0 24px 12px #CCCCCC; /* 页边来一个阴影！！好耶 */
    }
}

@media print {
    #write {
        padding: 0 !important;
    }
    #write a {
        color: inherit;
        text-decoration: none;
    }
    @page {
        /* size: A4;  强制A4大小 */
        margin: 1.8cm 2cm 1.2cm 2cm !important; /* 页边距在这里！！！！！！！！！！！！！！！！ */
        /* 参阅 <https://github.com/Keldos-Li/typora-latex-theme/blob/main/README.md#特别注意> */
    }
}

#write p {
    text-align: left;
}

/*
#write figure:after{
  counter-increment: tableHead;
  content: "表" counter(tableHead) " ";
  text-align:center;
  width:100%;
  display:inline-block;
}*/
/* 图片，导出PDF时暂时无法显示 */
/*
#write .md-image:after{
  counter-increment: imgHead;
  content: "图" counter(imgHead) " ";
  text-align:center;
  width:100%;
  display:inline-block;
}
*/

/* basic样式采用一般的引言，具有左边框、左缩进 */
blockquote {
    font-style: normal;
    font-family: var(--quote-font), var(--base-Latin-font), var(--base-Chinese-font), -apple-system, serif;
    font-size: var(--quote-font-size);
    border-left: 3px solid grey;
    padding-left: 16px; /* 文字离左边框的距离 */
    padding-right: 20pt;
    margin-left: 20px; /* 左边框离页面边的距离 */
    color: grey;
}

/*border-width: 0.5pt 0;
  border-style:solid; 这一行本来使用了上下边框*/

/* 标题属性 */
#write h1,
#write h2,
#write h3,
#write h4,
#write h5,
#write h6 {
    font-weight: bold; /* 对 Windows 的修改 */
    font-family: var(--heading-Latin-font), var(--heading-Chinese-font), sans-serif;
    page-break-after: avoid !important;
}

#write h1 {
    font-family: var(--heading-Latin-font), var(--title-Chinese-font), sans-serif;
    /*font-weight: bold;*/
    text-align: center;
    column-span: all;
    font-size: var(--title-font-size);
}
#write h2 {
    font-family: var(--heading-Latin-font), var(--h2-Chinese-font), sans-serif;
    font-size: var(--h2-font-size);
}
#write h3 {
    font-family: var(--heading-Latin-font), var(--h3-Chinese-font), sans-serif;
    font-size: var(--h3-font-size);
    line-height: var(--h3-font-size);
}
#write h4 {
    font-family: var(--heading-Latin-font), var(--h4-Chinese-font), sans-serif;
    font-size: var(--h4-font-size);
    line-height: var(--h4-font-size);
}
#write h5 {
    font-family: var(--heading-Latin-font), var(--h5-Chinese-font), sans-serif;
    font-size: var(--h5-font-size);
    line-height: var(--h5-font-size);
}
#write h6 {
    font-family: var(--heading-Latin-font), var(--h6-Chinese-font), sans-serif;
    font-size: var(--h6-font-size);
    line-height: var(--h5-font-size); /* 没有写错，为了避免行距太小才这么写 */
}


/* 三线表 */
#write table {
    border-top: 1.2pt solid; /* 三线表第一条线宽度 */
    border-bottom: 1.2pt solid; /* 三线表第二条线宽度 */
    font-family: var(--table-font), var(--base-Latin-font), var(--base-Chinese-font), serif;
    /* font-size: var(--base-font-size); */
    text-align: center;
    page-break-inside: avoid;
    border-spacing: 6px;
    width: auto; /* 自动布局表格宽度，如果有时内容太紧建议直接加空格吧，我自己看不惯和页面等宽的大表格 */
    margin: 0 auto; /* 使表格默认居中；虽然这个代码不好，但好像没别的实现办法 */
}

#write table td {
    padding: 2px;
}

#write table tr {
    padding: 2px;
}

#write th {
    padding: 0px 6px;
}

/* 表格标题（首行）样式 */
#write thead {
    border-bottom: 0.5pt solid; /* 三线表表头的线 */
    font-family: var(--table-title-font), var(--heading-Latin-font), var(--heading-Chinese-font), serif !important;
    /* font-size: var(--base-font-size); */
    font-weight: var(--strong-weight);
}

hr {
  border-top: solid 0.7px #EEE;
}

img {
    page-break-inside: avoid; /* 避免图片在导出时被断开 */
}

/* ============ 多级列表样式 ============ */
ul {
    list-style: disc; /* 无序列表第一级：实心圆点 */
}

ul ul {
    /*list-style: circle;*/
    list-style: '–　 '; /* 直接用空格代替一部分缩进的调整了我真懒 */
    /*left: -0.8em;*/
}

ul ul ul {
    list-style: "◦　  "; /* 无序列表第三极：小圈 */
    left: 0.6em;
}

ol {
    list-style: decimal;
    /* 有序列表第一级：数字 */
}

ol ol {
    counter-reset: liist;
    list-style: none;
}
ol ol li {
    counter-increment: liist;
    position: relative;
}
ol ol li::before {
    content: "("counter(liist, lower-alpha) ")　　"; /* 有序列表第二级：括号加小写字母 */
    position: absolute;
    margin-right: 2rem;
    left: -2.4em;
}

ol ol ol {
    counter-reset: liiist;
    list-style: none;
    margin: 0;
}
ol ol ol li {
    counter-increment: liiist;
    position: relative;
}
ol ol ol li::before {
    content: counter(liiist, lower-roman) ".　　　"; /* 有序列表第三级：小写罗马数字 这些后续还要改的可恶*/
    align-self: flex-end;
    position: absolute;
    left: -2.4em;
    /* -moz-box-sizing: border-box;
    -webkit-box-sizing: border-box;
    box-sizing: border-box;*/
    width: 4em; /* 为了让项目编号是重新用句点对齐而不是左对齐我调试了一整个晚上啊啊啊啊 CSS好难写 */
    text-align: right;
}

li {
    position: relative;
}
/* ============ 多级列表样式END ============ */


/* 行内代码 */
p code,
li code {
    color: rgb(60, 112, 198);
    background-color: #fefefe;
    font-family: var(--code-font), var(--ui-font), monospace;
    box-sizing: border-box;
    border-right: 0px;
    margin: 0 2px 0 2px;
    padding: 0 2px 0 2px;
    border-radius: 2px 2px 2px 2px; /* 圆角 */
    box-shadow: 0 0 1px 1px #c8d3df; /* 阴影 */
}

/* 代码块样式*/
#write .CodeMirror-wrap {
    padding: 10px; /* 当代码太长跨页的时候好像也会有点问题，到时候再改 */
}

#write .CodeMirror-code pre {
    font-family: var(--code-font), var(--ui-font), monospace;
}

/* typora 编写模式 */
#typora-source {
    font-family: var(--sourceMode-font), var(--code-font), var(--ui-font), monospace;
    line-height: 2rem;
}

/* 下面是标题自动编号，初始化计数器。使用多级编号，编号后加空格模仿LaTeX */
/* 首先全局进行一次reset，这样即使不添加h1标题也可以使用较低级别的标题 */
#write {
    counter-reset: h2 0 h3 0 h4 0 h5 0 h6 0
}

#write h1 {
    counter-reset: h2
}
h2 {
    counter-reset: h3
}
h3 {
    counter-reset: h4
}
h4 {
    counter-reset: h5
}
h5 {
    counter-reset: h6
}

/* put counter result into headings */
#write h2:before {
    counter-increment: h2;
    content: counter(h2);
    margin-right: 2rem;
}
/** override the default style for focused headings */
#write h3:before,
h3.md-focus.md-heading:before {
    counter-increment: h3;
    content: counter(h2) "."counter(h3);
    margin-right: 2rem;
}
#write h4:before,
h4.md-focus.md-heading:before {
    counter-increment: h4;
    content: counter(h2) "."counter(h3) "."counter(h4);
    margin-right: 2rem;
}
#write h5:before,
h5.md-focus.md-heading:before {
    counter-increment: h5;
    content: counter(h2) "."counter(h3) "."counter(h4) "."counter(h5);
    margin-right: 2rem;
}
#write h6:before,
h6.md-focus.md-heading:before {
    counter-increment: h6;
    content: counter(h2) "."counter(h3) "."counter(h4) "."counter(h5) "."counter(h6);
    margin-right: 2rem;
}

/* 目录 */
.md-toc-content {
    margin-left: 2em;
    counter-reset: toc-h2 toc-h3 toc-h4; /* 修复缺失上级标题时无法递增 */
    page-break-after: always;
}
.md-toc-inner {
    margin-left: 0 !important;
    color: black !important;
}
.md-toc-item {
    color: black !important;
}

/* 目录标题内容属性 */
.md-toc-h2,
.md-toc-h3,
.md-toc-h4,
.md-toc-h5,
.md-toc-h6 {
    font-size: var(--toc-font-size);
    font-family: var(--toc-font), var(--base-Latin-font), var(--base-Chinese-font), serif;
}
.md-toc-h2 {
    font-weight: var(--strong-weight);
}

/* 目录标题前 */
.md-toc-content .md-toc-h1 {
    display: var(--toc-show-title);
    counter-reset: toc-h2;
}
.md-toc-content .md-toc-h2 {
    counter-reset: toc-h3;
}
.md-toc-content .md-toc-h3 {
    counter-reset: toc-h4;
}
.md-toc-content .md-toc-h4 {
    counter-reset: toc-h5;
}
.md-toc-content .md-toc-h5 {
    counter-reset: toc-h6;
}
.md-toc-content .md-toc-h2:before {
    counter-increment: toc-h2;
    content: counter(toc-h2);
    margin-right: 1rem;
    font-weight: var(--strong-weight);
}
.md-toc-content .md-toc-h3:before {
    counter-increment: toc-h3;
    content: counter(toc-h2) "."counter(toc-h3);
    margin-left: 1.5rem;
    margin-right: 0.5rem;
}
.md-toc-content .md-toc-h4:before {
    counter-increment: toc-h4;
    content: counter(toc-h2) "."counter(toc-h3) "."counter(toc-h4);
    margin-left: 3.5rem;
    margin-right: 0.5rem;
}
.md-toc-content .md-toc-h5:before {
    counter-increment: toc-h5;
    content: counter(toc-h2) "."counter(toc-h3) "."counter(toc-h4) "."counter(toc-h5);
    margin-left: 5.5rem;
    margin-right: 0.5rem;
}
.md-toc-content .md-toc-h6:before {
    counter-increment: toc-h6;
    content: counter(toc-h2) "."counter(toc-h3) "."counter(toc-h4) "."counter(toc-h5) "."counter(toc-h6);
    margin-left: 7.5rem;
    margin-right: 0.5rem;
}

/* 侧边大纲标题 */
.sidebar-content .outline-h1 {
    counter-reset: outline-h2;
}
.sidebar-content .outline-h2 {
    counter-reset: outline-h3;
}
.sidebar-content .outline-h3 {
    counter-reset: outline-h4;
}
.sidebar-content .outline-h4 {
    counter-reset: outline-h5;
}
.sidebar-content .outline-h5 {
    counter-reset: outline-h6;
}
.sidebar-content .outline-h2 .outline-label:before {
    counter-increment: outline-h2;
    content: counter(outline-h2) " ";
}
.sidebar-content .outline-h3 .outline-label:before {
    counter-increment: outline-h3;
    content: counter(outline-h2) "."counter(outline-h3) "  ";
}
.sidebar-content .outline-h4 .outline-label:before {
    counter-increment: outline-h4;
    content: counter(outline-h2) "."counter(outline-h3) "."counter(outline-h4) "  ";
}
.sidebar-content .outline-h5 .outline-label:before {
    counter-increment: outline-h5;
    content: counter(outline-h2) "."counter(outline-h3) "."counter(outline-h4) "."counter(outline-h5) "  ";
}

.sidebar-content {
    font-family: var(--ui-font); /* 侧边栏的字体修改 */
    list-style: none;
}

/* 元数据（如YAML front matter）的背景框 */
pre.md-meta-block {
    background: #CCCCCC;
    padding: 1.4em;
    font-family: var(--code-font), var(--ui-font), monospace;
}

/** override the default style for focused headings */
#write>h3.md-focus:before,
#write>h4.md-focus:before,
#write>h5.md-focus:before,
#write>h6.md-focus:before,
h3.md-focus:before,
h4.md-focus:before,
h5.md-focus:before,
h6.md-focus:before {
    color: inherit;
    border: inherit;
    border-radius: inherit;
    position: inherit;
    left: initial;
    float: none;
    top: initial;
    font-size: inherit;
    padding-left: inherit;
    padding-right: inherit;
    vertical-align: inherit;
    font-weight: inherit;
    line-height: inherit;
}


</style>
</head>
<body class='typora-export os-windows'>
<div id='write'  class=' first-line-indent'><h1><a name="hadoop" class="md-header-anchor"></a><span>Hadoop</span></h1><h2><a name="概论" class="md-header-anchor"></a><span>概论</span></h2><blockquote><ul><li><span>面向数据存储和加工的平台</span></li><li><span>定义好编程模型和接口</span></li><li><span>用户编写Application，提交到平台：服务App vs. 运算App</span></li><li><span>开源，面向数据分析</span></li><li><span>不一定要在云上：分布式编程模型</span></li></ul></blockquote><h3><a name="问题" class="md-header-anchor"></a><span>问题</span></h3><h4><a name="存储" class="md-header-anchor"></a><span>存储</span></h4><ul><li><span>单机容量足够，各机器上的文件要求对外显示它们存于同一硬盘空间；</span></li><li><span>文件大小超出单机容量，要求利用多台机器存入后对外显示依旧为一个完整文件；</span></li></ul><h4><a name="计算" class="md-header-anchor"></a><span>计算</span></h4><blockquote><p><span>比如说数单词</span></p></blockquote><h4><a name="可靠性" class="md-header-anchor"></a><span>可靠性</span></h4><p><span>应对宕机</span></p><blockquote><p><span>网络开销、利用率、成本</span></p></blockquote><h3><a name="分布式解决方案" class="md-header-anchor"></a><span>分布式解决方案</span></h3><h4><a name="冗余存储与冗余计算" class="md-header-anchor"></a><span>冗余存储与冗余计算</span></h4><ul><li><span>解决可靠性问题——不单纯靠额外增加设备的备份</span></li><li><span>将每台机器上存储的数据同时存于集群中的另一台机器上</span></li><li><span>将每台机器上数据的计算也同时在冗余数据的机器上计算</span></li><li><code>CMaster0</code><span>明确知道每一份数据都存储在多个地方</span></li><li><code>CMaster1</code><span>会要求存有待计算数据的机器都参与计算，并选择先结束的机器计算结果</span></li></ul><p><span>冗余存储不仅提高了分布式存储的可靠性，也提高了分布式计算的可靠性。</span></p><blockquote><p><span>分布式存储和分布式计算可以相互独立存在</span></p></blockquote><h4><a name="分布式存储distributed-file-system-dfs）" class="md-header-anchor"></a><span>分布式存储（Distributed File System, DFS）</span></h4><p><span>将多台机器硬盘以某种方式连接到一起</span></p><p><span>取机器</span><code>cSlave0</code><span>，</span><code>cSlave1</code><span>……和</span><code>cMaster0</code><span>，采用客户-服务器模式构建分布式存储集群。</span></p><p><span>让</span><code>cMaster0</code><span>管理</span><code>cSlave0</code><span>，</span><code>cSlave1</code><span>……。</span></p><h5><a name="对内客户-服务器模式" class="md-header-anchor"></a><span>对内：客户-服务器模式</span></h5><p><span>只要保证store master正常工作，我们很容易随意</span></p><p><span>添加store slave，硬盘存储空间无限大。</span></p><h5><a name="对外统一存储空间统一文件接口" class="md-header-anchor"></a><span>对外：统一存储空间，统一文件接口</span></h5><p><span>整个集群就像是一台机器、一片云，硬盘显示为</span></p><p><span>统一存储空间，文件接口统一。</span></p><h4><a name="分布式计算" class="md-header-anchor"></a><span>分布式计算</span></h4><blockquote><p><span>“移动计算比移动数据更划算”——Google论文</span></p></blockquote><h5><a name="计算的并行-map" class="md-header-anchor"></a><span>计算的并行-</span><code>Map</code></h5><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015092645930.png" referrerpolicy="no-referrer" alt="image-20211015092645930"></p><h5><a name="合并的并行-reduce" class="md-header-anchor"></a><span>合并的并行-</span><code>Reduce</code></h5><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015092905250.png" referrerpolicy="no-referrer" alt="image-20211015092905250"></p><blockquote><p><span>经过洗牌之后，合并的时候分布的文本某一个特定单词的计数都送到某一个节点上进行合并，不同单词之间的合并技术可以并行</span></p></blockquote><h3><a name="hadoop简介" class="md-header-anchor"></a><span>Hadoop简介</span></h3><h4><a name="渊源" class="md-header-anchor"></a><span>渊源</span></h4><ul><li><span>Apache成立开源搜索引擎项目</span><code>Nutch</code><span>——但开发过程中无法有效地将计算任务分配到多台机器上</span></li><li><span>前后Google陆续发表</span><code>GFS</code><span>、</span><code>MapReduce</code><span>、</span><code>BigTable</code><span>（谷歌三板斧）</span></li><li><span>Apache借鉴GFS和MapReduce，实现了自己的</span><code>NDFS</code><span>和</span><code>MapReduce</code></li><li><span>发现</span><code>Nutch</code><span>侧重搜索，而</span><code>NDFS</code><span>和</span><code>MapReduce</code><span>偏向通用基础架构，将</span><code>NDFS</code><span>和</span><code>MapReduce</code><span>移出</span><code>Nutch</code><span>，成为独立开发项目，称为</span><code>Hadoop</code></li></ul><p><span>Hadoop 1.0 （1.X的统称）和Hadoop 2.0 （2.X的统称）架构差异较大。</span></p><h4><a name="简介" class="md-header-anchor"></a><span>简介</span></h4><blockquote><p><span>可看作是Google Cloud的开源版本；但并不拘泥于复现Google Cloud的相关产品。</span></p><figure><table><thead><tr><th><strong><span>Hadoop</span></strong></th><th><span>Google</span></th><th><span>描述</span></th></tr></thead><tbody><tr><td><span>Hadoop  HDFS</span></td><td><span>Google  GFS</span></td><td><span>分布式文件系统</span></td></tr><tr><td><span>Hadoop  MapReduce</span></td><td><span>Google  MapReduce</span></td><td><span>分布式计算</span></td></tr><tr><td><span>HBase</span></td><td><span>Google  BigTable</span></td><td><span>分布式数据库</span></td></tr><tr><td><span>ZooKeeper</span></td><td><span>Google Chubby</span></td><td><span>消息队列</span></td></tr><tr><td><span>Pig</span></td><td><span>Google  Sawzall</span></td><td><span>脚本语言</span></td></tr></tbody></table></figure></blockquote><p><span>通过调用程序库，可使用简单的编程模型处理分布在不同机器上的大规模数据。</span></p><p><span>采用客户-服务器模式，很容易从一台机器扩展至成千上万台机器，每台机器均能提供本地存储和本地计算。</span></p><h5><a name="hadoop-10" class="md-header-anchor"></a><span>Hadoop 1.0</span></h5><ul><li><code>Hadoop Common</code><span>：支持其他两个模块的公用组件</span></li><li><code>Hadoop DFS</code><span>（</span><code>HDFS</code><span>）：分布式文件系统</span></li><li><code>Hadoop MapReduce</code><span>：分布式计算框架</span></li></ul><h5><a name="hadoop-20" class="md-header-anchor"></a><span>Hadoop 2.0</span></h5><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015170805021.png" referrerpolicy="no-referrer" alt="image-20211015170805021"></p><ul><li><p><span>分布式操作系统</span><code>Yarn</code></p></li><li><p><code>ZooKeeper</code><span>：通用的分布式集群的数据管理者</span></p><p><span>不仅仅只是为Hadoop服务！集群化思想</span></p><ul><li><span>统一命名服务</span></li><li><span>配置管理</span></li><li><span>集群管理</span></li></ul></li><li><p><code>Hbase</code><span>：开源分布式数据库</span></p><ul><li><span>高可靠性</span></li><li><span>高性能</span></li><li><span>列存储</span></li><li><span>可伸缩</span></li><li><span>实时读写</span></li></ul><p><strong><span>逻辑模型</span></strong><span>：用户对数据的组织形式</span></p><p><code>行键、列（&lt;列族&gt;:&lt;限定符&gt;）、时间戳；字节码，无类型</code></p><p><strong><span>物理模型</span></strong><span>：在设备上具体存储形式</span></p><p><span>将行按照列族分割存储；逻辑空值无存储</span></p></li></ul><h4><a name="应用领域" class="md-header-anchor"></a><span>应用领域</span></h4><h5><a name="构建大型分布式集群存储计算）" class="md-header-anchor"></a><span>构建大型分布式集群（存储+计算）</span></h5><ul><li><span>最直接的应用</span></li><li><span>构建大型分布式集群，提供海量存储和计算服务</span></li><li><span>类似产品中国移动“大云”、淘宝“云梯”</span></li></ul><h5><a name="数据仓库存储）" class="md-header-anchor"></a><span>数据仓库（存储）</span></h5><p><span>存储半结构化业务数据，通过</span><code>Hive</code><span>、</span><code>Hbase</code><span>提供报表查询之类服务</span></p><h5><a name="数据挖掘计算）" class="md-header-anchor"></a><span>数据挖掘（计算）</span></h5><ul><li><span>大数据环境下的数据挖掘思路和算法没有太大变化</span></li><li><span>硬盘性能和内存大小带来的限制——通过分布式集群解决硬件限制</span></li></ul><h4><a name="部署方式" class="md-header-anchor"></a><span>部署方式</span></h4><p><span>传统解压包（繁琐易错）和标准Linux部署方式（简单易用）</span></p><blockquote><p><span>这里会有一个专门的作业，</span></p></blockquote><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015095217686.png" referrerpolicy="no-referrer" alt="image-20211015095217686"></p><blockquote><p><span>注意下面均以Hadoop 2.0为基础</span></p></blockquote><h2><a name="体系架构" class="md-header-anchor"></a><span>体系架构</span></h2><h3><a name="common" class="md-header-anchor"></a><span>Common</span></h3><p><span>其他模块的公共组件，定义程序员取得集群服务的编程接口，为其他模块提供共用API。</span></p><h4><a name="作用" class="md-header-anchor"></a><span>作用</span></h4><p><span>降低Hadoop设计的复杂性，减少其他模块间耦合性，增强Hadoop健壮性。</span></p><h4><a name="功能" class="md-header-anchor"></a><span>功能</span></h4><ul><li><span>提供公用API和程序员编程接口（例如Configuration类）；</span></li><li><span>本地Hadoop库（例如压缩解压缩用的是Hadoop本地库）；</span></li><li><span>超级用户superuser；</span></li><li><span>服务级别认证；</span></li><li><span>HTTP认证；</span></li></ul><h3><a name="hdfs" class="md-header-anchor"></a><span>HDFS</span></h3><p><span>高容错、高扩展、高可靠，并提供服务访问接口，如API接口和管理员接口。</span></p><h4><a name="体系架构-n974" class="md-header-anchor"></a><span>体系架构</span></h4><ul><li><span>架构：</span><code>master</code><span>/</span><code>slave</code><span>；文件分块存储；</span><code>namenode</code><span>/</span><code>datanode</code><span>；</span></li><li><span>典型拓扑：一般/商用（</span><code>ZooKeeper</code><span>选举</span><code>ActiveNamenode</code><span>，</span><code>JourNalNode</code><span>两个</span><code>Namenode</code><span>交换数据）</span></li></ul><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015153417530.png" referrerpolicy="no-referrer" alt="image-20211015153417530"></p><h4><a name="内部特性" class="md-header-anchor"></a><span>内部特性</span></h4><ul><li><span>冗余备份、副本存放、副本选择、心跳检测</span></li><li><span>数据完整性检测、元数据磁盘失效</span></li><li><span>简单一致性模型、流式数据访问、客户端缓存</span></li><li><span>流水线复制、架构特征、超大规模数据集</span></li></ul><h4><a name="对外功能" class="md-header-anchor"></a><span>对外功能</span></h4><ul><li><code>Namenode</code><mark><span>高可靠性</span></mark><span>：配置多个</span><code>NameNode</code><span>，一个失效时立即替换</span></li><li><code>HDFS</code><span>快照：当数据损坏时，支持回滚到正确的时间节点</span></li><li><span>元数据管理与恢复工具：通过命令</span><code>hdfs oiv</code><span>和</span><code>hdfs oev</code><span>管理修复</span><code>fsimage</code><span>和</span><code>edits</code></li><li><code>HDFS</code><span>安全性：用户和文件级别安全认证、机器和服务级别安全认证</span></li><li><code>HDFS</code><span>配额功能：管理目录或文件配额大小</span></li><li><code>HDFS</code><span> C语言接口：使用C语言操作HDFS的接口</span></li><li><code>HDFS Short-Circuit</code><span>功能：客户端可以绕开</span><code>Datanode</code><span>直接读取本机数据，加快</span><code>Map</code><span>操作</span></li><li><code>WebHdfs</code><span>：</span><mark><span>通过</span><code>Web</code><span>方式操作</span><code>HDFS</code></mark><span>（插、删、改、查）</span></li></ul><h3><a name="yarn" class="md-header-anchor"></a><span>Yarn</span></h3><p><span>管理计算机资源、提供用户和程序访问系统资源的API。</span></p><ul><li><span>一个高层的集群管理框架；</span></li><li><span>根据需要的计算类型，定制</span><code>ApplicationMaster</code><span>；</span></li><li><span>根据需要的调度策略，扩展</span><code>Scheduler</code></li></ul><h4><a name="架构" class="md-header-anchor"></a><span>架构</span></h4><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015154235691.png" referrerpolicy="no-referrer" alt="image-20211015154235691"></p><h4><a name="组件详解" class="md-header-anchor"></a><span>组件详解</span></h4><ol start='' ><li><code>Client</code><span>：客户端，负责向集群提交作业。</span></li><li><code>ResourceManager</code><span>：集群主进程，仲裁中心，负责集群资源管理和任务调度。</span></li><li><code>Scheduler</code><span>：资源仲裁模块。</span><mark><span>纯粹的资源仲裁中心</span></mark></li><li><code>ApplicationManager</code><span>：选定，启动和</span><mark><span>只</span></mark><span>监管</span><code>ApplicationMaster</code><span>。</span></li><li><code>NodeManager</code><span>：集群从进程，管理监视</span><code>Containers</code><span>，执行具体任务。</span></li><li><code>Container</code><span>：</span><mark><span>本机</span></mark><span>资源集合体，如某</span><code>Container</code><span>为4个CPU，8GB内存。</span></li><li><code>ApplicationMaster</code><span>：任务执行和监管中心。</span><mark><span>负责任务整体执行</span></mark></li></ol><h4><a name="作业流程" class="md-header-anchor"></a><span>作业流程</span></h4><ol start='' ><li><span>提交作业</span></li><li><span>任务分配</span></li><li><span>任务执行</span></li><li><span>进度和状态更新</span></li><li><span>任务完成</span></li></ol><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015154901595.png" referrerpolicy="no-referrer" alt="image-20211015154901595"></p><h4><a name="典型拓扑" class="md-header-anchor"></a><span>典型拓扑</span></h4><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015155031593.png" referrerpolicy="no-referrer" alt="image-20211015155031593"></p><h4><a name="核心问题调度策略" class="md-header-anchor"></a><span>核心问题：调度策略</span></h4><p><code>ResourceManager</code><span>的</span><code>Scheduler</code><span>模块支持插拔，通过配置文件，用户可以个性化指定其调度策略。</span></p><h5><a name="自带策略1容量调度算法capacityscheduler" class="md-header-anchor"></a><span>自带策略1：容量调度算法</span><code>CapacityScheduler</code></h5><ul><li><span>多用户多任务调度策略；</span></li><li><span>以队列为单位划分任务，以Container为单位分配资源；</span></li><li><span>按照配置好的资源配比为不同层级的用户分配最大可用资源；</span></li></ul><h5><a name="自带策略2公平调度算法fairscheduler" class="md-header-anchor"></a><span>自带策略2：公平调度算法</span><code>FairScheduler</code></h5><ul><li><span>多任务公平使用集群资源的可插拔式调度策略；</span></li><li><span>当资源能够满足所有任务时，则按需分配资源；</span></li><li><span>当资源受限时，会将正在执行的任务释放的资源分配给在等待资源的任务；</span></li><li><span>短任务在合理时间内完成；长任务不至于永远等待；</span></li></ul><h3><a name="mapreduce" class="md-header-anchor"></a><span>MapReduce</span></h3><p><span>编程范式。</span></p><p><code>Yarn</code><span>中</span><code>ApplicationMaster</code><span>用来管理任务的执行，其能够管理的任务类型是固定的</span></p><p><span>通过定义不同类型的</span><code>ApplicationMaster</code><span>，可以实现管理不同类型的任务。可以将</span><code>MapReduce</code><span>看作一种类型的计算任务。提供对应的</span><code>ApplicationMaster</code><span>来管理</span><code>MapReduce</code><span>任务。</span></p><p><mark><code>ApplicationMaster</code><span>和</span><code>Scheduler</code><span>都是可变的</span></mark><span>。</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015164310161.png" referrerpolicy="no-referrer" alt="image-20211015164310161"></p><h3><a name="拓展" class="md-header-anchor"></a><span>拓展</span></h3><ul><li><span>在Hadoop框架内定义新的计算任务——编程模板</span></li><li><span>跳出Hadoop的限制——</span><mark><span>Spark</span></mark><span>——定义更加通用的编程</span></li><li><span>安全机制——不同级别、不同场景的安全认证</span></li></ul><h2><a name="访问接口" class="md-header-anchor"></a><span>访问接口</span></h2><h3><a name="web" class="md-header-anchor"></a><span>Web</span></h3><figure><table><thead><tr><th><span>服务</span></th><th><span>Web地址</span></th><th><span>配置文件</span></th><th><span>配置参数</span></th></tr></thead><tbody><tr><td><code>HDFS</code></td><td><code>http://&lt;NameNodeHostName&gt;:50070</code></td><td><code>hdfs-site.xml</code></td><td><code>{dfs.namenode.http-address}</code></td></tr><tr><td><code>Yarn</code></td><td><code>http://&lt;ResourceManagerHostName&gt;:8088</code></td><td><code>yarn-site.xml</code></td><td><code>{yarn.resourcemanager.webapp.address}</code></td></tr><tr><td><code>MapReduce</code></td><td><code>http://&lt;JobHistoryHostName&gt;:19888</code></td><td><code>mapred-site.xml</code></td><td><code>{mapreduce.jobhistory.webapp.address}</code></td></tr></tbody></table></figure><h3><a name="命令行" class="md-header-anchor"></a><span>命令行</span></h3><h4><a name="hdfs命令" class="md-header-anchor"></a><span>HDFS命令</span></h4><ul><li><span>以tar包方式部署时，其执行方式是</span><code>$HADOOP_HOME/bin/hdfs</code></li><li><span>以完全模式部署时，使用HDFS用户执行</span><code>hdfs</code><span>即可</span></li></ul><h4><a name="yarn命令" class="md-header-anchor"></a><span>Yarn命令</span></h4><ul><li><span>以tar包方式部署时，其执行方式是</span><code>$HADOOP_HOME/bin/yarn</code></li><li><span>以完全模式部署时，使用</span><code>Yarn</code><span>用户执行</span><code>yarn</code><span>即可</span></li></ul><h4><a name="hadoop命令" class="md-header-anchor"></a><span>Hadoop命令</span></h4><ul><li><span>两种部署方式下分别为</span><code>$HADOOP_HOME/bin/Hadoop</code><span> 和 </span><code>hadoop</code></li></ul><h4><a name="其他" class="md-header-anchor"></a><span>其他</span></h4><ul><li><p><code>sbin/</code><span>目录下的脚本</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015165528143.png" referrerpolicy="no-referrer" alt="image-20211015165528143"></p></li><li><p><span>启停服务/管理服务</span></p></li></ul><h3><a name="开发接口" class="md-header-anchor"></a><span>开发接口</span></h3><h4><a name="hdfs编程" class="md-header-anchor"></a><span>HDFS编程</span></h4><ul><li><span>实例化配置文件 </span><code>Configuration conf = new Configuration()</code><span>；</span></li><li><span>获取文件系统 </span><code>FileSystem hdfs = FileSystem.get(conf)</code><span>；</span></li><li><span>使用</span><code>hdfs</code><span>对象执行文件操作；</span></li></ul><blockquote><p><a href='https://hadoop.apache.org/docs/r2.6.4/api/org/apache/hadoop/conf/Configuration.html' target='_blank' class='url'>https://hadoop.apache.org/docs/r2.6.4/api/org/apache/hadoop/conf/Configuration.html</a></p><p><span>通过configuration传参：</span><code>conf.set(key, value)</code><span>，</span><code>context.getConfiguration().getInt(key)</code></p></blockquote><h4><a name="yarn编程" class="md-header-anchor"></a><span>Yarn编程</span></h4><ul><li><span>一套编程协议；</span></li><li><code>Client</code><span>负责提交任务，</span><code>ApplicationMaster</code><span>负责执行任务；</span></li><li><span>Client中与RM通信；</span><code>ApplicationMaster</code><span>与RM通信；</span><code>ApplicationMaster</code><span>与NM通信</span></li><li><span>编写符合协议的</span><code>Client</code><span>和</span><code>ApplicationMaster</code><span>即可</span></li></ul><h4><a name="只需考虑mapreduce本身" class="md-header-anchor"></a><span>只需考虑MapReduce本身</span></h4><p><code>Hadoop</code><span>默认实现了</span><code>MapReduce</code><span>的</span><code>Client</code><span>和</span><code>ApplicationMaster</code><span>、</span><code>MRClientService</code><span>和</span><code>MRAppMaster</code><span>等。</span></p><p><code>Yarn</code><span>处理MR程序时使用了各种默认的类。</span></p><h1><a name="spark" class="md-header-anchor"></a><span>Spark</span></h1><h2><a name="概论-n1158" class="md-header-anchor"></a><span>概论</span></h2><h3><a name="简介-n1159" class="md-header-anchor"></a><span>简介</span></h3><p><span>当今大数据领域最活跃、最热门的大数据计算处理框架</span></p><ul><li><span>2009年——诞生于美国加州大学伯克利分校AMP实验室</span></li><li><span>2013年——Spark成为</span><code>Apache</code><span>基金项目</span></li><li><span>2014年——成为</span><code>Apache</code><span>基金顶级项目</span></li></ul><p><span>一体化、多元化的大数据处理体系</span></p><ul><li><span>批处理，</span><code>Batch Processing</code></li><li><span>流处理，</span><code>Stream Processing</code></li><li><span>即席查询，</span><code>Adhoc Query</code></li></ul><h4><a name="开发包" class="md-header-anchor"></a><span>开发包</span></h4><p><code>Spark SQL</code><span>、</span><code>Spark Streaming</code><span>、</span><code>Spark Mllib</code><span>、</span><code>Spark GraphX</code><span>……</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015173939819.png" referrerpolicy="no-referrer" alt="image-20211015173939819"></p><h5><a name="spark-sql" class="md-header-anchor"></a><code>Spark SQL</code></h5><ul><li><span>前身是Shark：基于Hive的Spark SQL，代码量大、复杂，难优化和维护</span></li><li><span>交互式查询、标准访问接口、兼容Hive</span></li><li><span>专门用于处理结构化数据：</span></li><li><span>分布式SQL引擎；在Spark程序中调用API</span></li></ul><h4><a name="spark-streaming" class="md-header-anchor"></a><code>Spark Streaming</code></h4><ul><li><span>实时对大量数据进行快速处理，处理周期短</span></li><li><span>对数据分段、定义了自动监听更新的框架、提供各种Spark计算函数</span></li></ul><h5><a name="spark-graphx" class="md-header-anchor"></a><code>Spark GraphX</code></h5><ul><li><span>以图为基础数据结构的算法实现和相关应用</span></li><li><span>使用RDD存储图节点和边信息，提供各种计算函数</span></li></ul><h5><a name="spark-mllib" class="md-header-anchor"></a><code>Spark MLlib</code></h5><ul><li><span>为解决机器学习开发的库</span></li></ul><p><span>包括</span><code>分类</code><span>、</span><code>回归</code><span>、</span><code>聚类</code><span>和</span><code>协同过滤</code><span>等。</span></p><blockquote><p><span>插曲，</span><code>Spark Mllib</code><span>的效率被基于</span><code>MPI</code><span>（</span><code>Message Passing Interface</code><span>）的机器学习吊打</span></p><p><span>可能的解释：</span><a href='https://zhuanlan.zhihu.com/p/81784947' target='_blank' class='url'>https://zhuanlan.zhihu.com/p/81784947</a></p></blockquote><p><code>Spark</code><mark><span>专注于数据的计算</span></mark><span>，而</span><mark><span>数据的存储</span></mark><span>在生产环境中往往还是由Hadoop分布式文件系统</span><mark><span>HDFS</span></mark><span>承担。</span></p><h3><a name="优势" class="md-header-anchor"></a><span>优势</span></h3><h4><a name="对比hadoop" class="md-header-anchor"></a><span>对比Hadoop</span></h4><ul><li><p><span>支持多种数据计算需求</span></p><ul><li><span>流式迭代的类MR计算</span></li><li><span>图数据结构的计算</span></li></ul></li><li><p><span>基于内存的计算范式，不用像</span><code>Hadoop</code><span>那样需要不停地写入硬盘（落盘）</span></p></li></ul><blockquote><p><code>Spark</code><span>曾经是一个</span><code>Hadoop</code><span>应用程序，但是</span><code>Spark</code><span>并不一定要依赖于</span><code>Hadoop</code></p></blockquote><h4><a name="对比mapreduce" class="md-header-anchor"></a><span>对比MapReduce</span></h4><p><code>Spark</code><span>是在</span><code>Hadoop</code><span>开创的分布式计算框架下对</span><code>MapReduce</code><span>编程范式进行扩展的一种更加通用的并行计算框架。</span></p><ul><li><span>独立性更强</span></li><li><span>基于内存：RDD，速度更快；基于内存的计算快100x倍，基于硬盘的快10x倍</span></li><li><span>支持更多数据计算方法：</span><code>transformation</code><span>，</span><code>action</code><span>等</span></li></ul><h4><a name="整体优势" class="md-header-anchor"></a><span>整体优势</span></h4><h5><a name="易用" class="md-header-anchor"></a><strong><span>易用</span></strong></h5><p><span>  支持</span><code>Scala</code><span>、</span><code>Java</code><span>、</span><code>Python</code><span>和</span><code>R</code><span>等多种编程语言</span></p><h5><a name="强大" class="md-header-anchor"></a><strong><span>强大</span></strong></h5><p><span>  支持</span><code>SQL</code><span>、</span><code>Streaming</code><span>、</span><code>Graph</code><span>和</span><code>Machine Learning</code><span>等多种应用场景</span></p><h5><a name="通用" class="md-header-anchor"></a><strong><span>通用</span></strong></h5><p><span>  适用于自带的</span><code>Standalone</code><span>、</span><code>Mesos</code><span>、</span><code>Yarn</code><span>等多种不同分布式集群管理框架；</span></p><p><span>  适用多种不同数据存储方式（数据读取接口丰富）</span></p><h3><a name="部署模式" class="md-header-anchor"></a><span>部署模式</span></h3><h4><a name="local模式" class="md-header-anchor"></a><span>Local模式</span></h4><p><code>Local</code><span>是方便初学者入门学习和测试用的部署模式</span></p><h4><a name="cluster模式" class="md-header-anchor"></a><span>Cluster模式</span></h4><p><span>Cluster是真正的集群模式</span></p><ul><li><code>Standalone</code><span>集群管理器</span></li><li><code>Yarn</code><span>集群管理器</span></li><li><code>Mesos</code><span>集群管理器</span></li></ul><h3><a name="提交模式" class="md-header-anchor"></a><span>提交模式</span></h3><p><code>Client</code><span>模式和</span><code>Cluster</code><span>模式</span></p><p><code>Local</code><span>部署模式只支持</span><code>Client</code><span>提交模式。</span></p><p><span>真正的</span><code>Cluster</code><span>集群部署模式同时支持</span><code>Client</code><span>和</span><code>Cluster</code><span>提交模式。</span></p><h4><a name="client提交模式" class="md-header-anchor"></a><span>Client提交模式</span></h4><p><span>在</span><code>worker</code><span>节点启动</span><code>Driver</code><span>程序运行应用程序，结果返回到</span><code>Client</code><span>端。</span></p><h4><a name="cluster提交模式" class="md-header-anchor"></a><span>Cluster提交模式</span></h4><p><span>在</span><code>Master</code><span>上启动</span><code>Driver</code><span>程序，结果不会返回给</span><code>Client</code><span>而是保存在</span><code>Master</code><span>上。</span></p><h2><a name="内核机制解析" class="md-header-anchor"></a><span>内核机制解析</span></h2><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015193231739.png" referrerpolicy="no-referrer" alt="image-20211015193231739"></p><h3><a name="rddresilient-distributed-datasets）" class="md-header-anchor"></a><code>RDD</code><span>（</span><code>Resilient Distributed Datasets</code><span>）</span></h3><p><span>Spark是建立在RDD(Resilient Distributed Datasets，弹性分布式数据集)之上的。</span></p><p><span>RDD是一个</span><mark><span>容错的</span></mark><span>、</span><mark><span>并行的</span></mark><span>逻辑数据结构，使得Spark可以用一致的方式处理大数据的不同应用场景。</span></p><h4><a name="性质" class="md-header-anchor"></a><span>性质</span></h4><p><span>RDD是高度受限的共享内存模型。</span></p><ul><li><span>RDD只能从外界接入或由其他RDD产生。</span></li><li><span>从父RDD到子RDD的过程可以构建一棵树（有向无环图，</span><code>Directed Acyclic Graph（DAG）</code><span>）。若子RDD出现问题，可以通过父RDD重新推演。</span></li></ul><h4><a name="特点" class="md-header-anchor"></a><span>特点</span></h4><ul><li><span>数据存储到内存和磁盘中</span></li><li><span>控制数据分区</span></li><li><span>丰富的API操作数据</span></li></ul><h4><a name="五大特性" class="md-header-anchor"></a><span>五大特性</span></h4><ul><li><span>分区列表：记录了数据块所在的分区位置；一个RDD对应的数据是切分为数据块存储在集群的不同节点上的。</span></li><li><span>依赖列表：记录了当前这个RDD依赖于哪些其它的RDD。</span></li><li><span>计算函数：用于计算RDD各个分区的值。</span></li><li><strong><span>可选</span></strong><span>：分区器，子类可以重新指定新的分区方式：Hash和Range。</span></li><li><strong><span>可选</span></strong><span>：计算各分区时优先的位置列表。例如从HDFS文件生成RDD时，HDFS文件所在位置就是对应生成的RDD分区所在位置的优先选择。</span></li></ul><h4><a name="两种算子" class="md-header-anchor"></a><span>两种算子</span></h4><h5><a name="transformation" class="md-header-anchor"></a><span>Transformation</span></h5><figure><table><thead><tr><th><strong><span>Transformation操作</span></strong></th><th><strong><span>说明</span></strong></th></tr></thead><tbody><tr><td><code>map(func)</code></td><td><span>对源</span><code>RDD</code><span>中的每个元素调用</span><code>func</code><span>，生产新的元素，这些元素构成新的RDD并返回</span></td></tr><tr><td><code>flatMap(func)</code></td><td><span>与</span><code>map</code><span>类似，但是</span><code>func</code><span>的返回是多个成员</span></td></tr><tr><td><code>filter(func)</code></td><td><span>对</span><code>RDD</code><span>成员进行过滤，成员调用</span><code>func</code><span>返回</span><code>True</code><span>的才保留，保留的构成新</span><code>RDD</code><span>返回</span></td></tr><tr><td><code>mapPartitions(func)</code></td><td><span>和</span><code>map</code><span>类似，但是</span><code>func</code><span>作用于整个分区，类型是</span><code>Iterator&lt;T&gt;  =&gt; Iterator&lt;T&gt;</code></td></tr><tr><td><code>mapPartitionsWithIndex(func)</code></td><td><span>…</span></td></tr><tr><td><code>union(otherDataset)</code></td><td><span>…</span></td></tr><tr><td><code>reduceByKey(func, [numTasks])</code></td><td><span>对</span><code>&lt;key,  value&gt;</code><span>结构的</span><code>RDD</code><span>聚合，相同</span><code>key</code><span>的</span><code>value</code><span>调用</span><code>reduce</code><span>，</span><code>func</code><span>是</span><code>(v,v)=&gt;v</code></td></tr><tr><td><code>join(otherDataset, [numTasks])</code></td><td><span>…</span></td></tr></tbody></table></figure><h5><a name="action" class="md-header-anchor"></a><span>Action</span></h5><figure><table><thead><tr><th><strong><span>Action操作</span></strong></th><th><strong><span>说明</span></strong></th></tr></thead><tbody><tr><td><code>reduce(func)</code></td><td><span>对RDD成员使用</span><code>func</code><span>进行</span><code>reduce</code><span>，</span><code>func</code><span>接收两个值返回一个值；</span><code>reduce</code><span>只有一个返回值。</span><code>func</code><span>会并发执行</span></td></tr><tr><td><code>collect()</code></td><td><span>将</span><code>RDD</code><span>读取到</span><code>Driver</code><span>程序里面，类型是</span><code>Array</code><span>，要求</span><code>RDD</code><span>不能太大</span></td></tr><tr><td><code>count()</code></td><td><span>返回</span><code>RDD</code><span>成员数量</span></td></tr><tr><td><code>first()</code></td><td><span>返回</span><code>RDD</code><span>第一个成员，等价于</span><code>take(1)</code></td></tr><tr><td><code>take(n)</code></td><td><span>…</span></td></tr><tr><td><code>saveAsTextFile(path)</code></td><td><span>把</span><code>RDD</code><span>转换成文本内容并保存到指定的</span><code>path</code><span>路径下，可能有多个文件；</span><code>path</code><span>可以是本地文件系统路径，也可以是</span><code>HDFS</code><span>路径，转换方法是</span><code>RDD</code><span>成员的</span><code>toString</code><span>方法</span></td></tr><tr><td><code>saveAsSequenceFile(path)</code></td><td><span>…</span></td></tr><tr><td><code>foreach(func)</code></td><td><span>对</span><code>RDD</code><span>的每个成员执行</span><code>func</code><span>方法，没有返回值</span></td></tr></tbody></table></figure><h4><a name="弹性的七个方面" class="md-header-anchor"></a><span>弹性的七个方面</span></h4><h5><a name="自动进行内存和磁盘存储的切换-效率考虑" class="md-header-anchor"></a><span>自动进行内存和磁盘存储的切换 </span><code>效率考虑</code></h5><p><span>优先内存，实在放不下则放到磁盘里。</span></p><h5><a name="基于lineage血统）的高效容错机制-效率考虑-容错考虑" class="md-header-anchor"></a><span>基于Lineage（血统）的高效容错机制 </span><mark><code>效率考虑</code><span> </span><code>容错考虑</code></mark></h5><p><span>记录每一个数据分片的计算来源，便于快速恢复。</span></p><h5><a name="task如果失败会自动进行特定次数的重试-容错考虑" class="md-header-anchor"></a><span>Task如果失败会自动进行特定次数的重试 </span><mark><code>容错考虑</code></mark></h5><p><code>TaskScheduler</code><span>获取一个</span><code>Stage</code><span>的</span><code>TaskSet</code><span>，运行它们；默认4次。</span></p><h5><a name="stage如果失败会自动进行特定次数的重试-容错考虑" class="md-header-anchor"></a><span>Stage如果失败会自动进行特定次数的重试 </span><mark><code>容错考虑</code></mark></h5><p><code>DAGScheduler</code><span>调度</span><code>Stage</code><span>，</span><code>Stage</code><span>跟踪执行情况；默认4次。</span></p><h5><a name="checkpoint和persist检查点和持久化）可主动或被动触发-效率考虑-容错考虑" class="md-header-anchor"></a><code>Checkpoint</code><span>和</span><code>Persist</code><span>（检查点和持久化）可主动或被动触发 </span><mark><code>效率考虑</code><span> </span><code>容错考虑</code></mark></h5><p><span>用户能见的</span><code>RDD</code><span>可以</span><mark><span>主动调用</span></mark><span>，自动产生的中间RDD则可</span><mark><span>配置</span></mark><span>。</span></p><h5><a name="数据调度弹性dagschedulertaskscheduler和资源管理无关）效率考虑" class="md-header-anchor"></a><span>数据调度弹性（</span><code>DAGScheduler</code><span>、</span><code>TaskScheduler</code><span>和资源管理无关）</span><mark><code>效率考虑</code></mark></h5><p><span>任务调度、数据分配和计算资源的管理解耦：Yarn管理Spark集群</span></p><h5><a name="数据分片的高度弹性合并分片和切分分片-效率考虑" class="md-header-anchor"></a><span>数据分片的高度弹性：合并分片和切分分片 </span><mark><code>效率考虑</code></mark></h5><p><span>根据产生的分片（数据块）大小自动选择继续切分还是放到磁盘</span></p><h4><a name="创建方式" class="md-header-anchor"></a><span>创建方式</span></h4><ul><li><span>通过已存在的数据集合创建（变量）</span></li><li><span>通过HDFS和本地文件系统创建</span><code>textFile(…)</code></li><li><span>通过其他RDD转换（</span><code>transformation</code><span>操作）</span></li><li><span>从数据库读入</span></li></ul><h4><a name="宽窄依赖" class="md-header-anchor"></a><span>宽窄依赖</span></h4><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015204733278.png" referrerpolicy="no-referrer" alt="image-20211015204733278"></p><figure><table><thead><tr><th style='text-align:center;' ><span>窄依赖（</span><code>NarrowDependency</code><span>）</span></th><th style='text-align:center;' ><span>宽依赖（</span><code>ShuffleDependency</code><span>）</span></th></tr></thead><tbody><tr><td style='text-align:center;' ><mark><span>父RDD</span></mark><span>中的Partition</span><mark><span>最多</span></mark><span>被</span><mark><span>子RDD</span></mark><span>的</span><mark><span>1个</span></mark><span>Partition所使用</span></td><td style='text-align:center;' ><mark><span>一个</span></mark><mark><span>父RDD</span></mark><span>的Partition会同时被</span><mark><span>子RDD</span></mark><span>的</span><mark><span>多个</span></mark><span>Partition所使用</span></td></tr><tr><td style='text-align:center;' ><mark><span>完全</span></mark><span>并行执行</span></td><td style='text-align:center;' ><span>需要</span><mark><span>Shuffle后</span></mark><span>才能对计算</span><mark><span>并行化</span></mark><span>；</span><mark><span>Shuffle过程不能完全并行</span></mark></td></tr><tr><td style='text-align:center;' ><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015205511434.png" referrerpolicy="no-referrer" alt="image-20211015205511434"></td><td style='text-align:center;' ><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015205517567.png" referrerpolicy="no-referrer" alt="image-20211015205517567"></td></tr></tbody></table></figure><h4><a name="运算流程" class="md-header-anchor"></a><span>运算流程</span></h4><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015213320153.png" referrerpolicy="no-referrer" alt="image-20211015213320153"></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015205800617.png" referrerpolicy="no-referrer" alt="image-20211015205800617"></p><p><span>构造DAG，划分</span><code>Job</code><span>、</span><code>Stage</code><span>、</span><code>Task</code><span>，</span><mark><span>遇到</span><code>Action</code><span>，才会提交</span><code>Job</code></mark><span>。</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211015210017829.png" referrerpolicy="no-referrer" alt="image-20211015210017829"></p><ul><li><code>Transformation</code><span>：从持久化存储中通过</span><em><span>变换</span></em><span>（</span><strong><span>Transformations</span></strong><span>，如 </span><em><span>map</span></em><span> 或者 </span><em><span>filter</span></em><span>）将其载入内存，然后可以对 RDD 施加任何系统支持的一系列变换。它只是一个声明，不碰到Action运算不会进行。</span></li><li><code>Action</code><span>：将 RDD 重新持久化到外存中或者将控制权交还用户。</span></li><li><code>Application</code><span>：客户端的一次提交，可以看作一个主函数。</span></li><li><code>Job</code><span>：由action触发的一系列计算任务。</span></li><li><code>Stage</code><span>：把一个</span><code>job</code><span>按照宽依赖分割成若干阶段。</span></li><li><code>Task</code><span>：把</span><code>stage</code><span>根据RDD分区数进行区分。</span></li></ul><blockquote><p><span>参考链接：</span><a href='https://stackoverflow.com/questions/42263270/what-is-the-concept-of-application-job-stage-and-task-in-spark' target='_blank' class='url'>https://stackoverflow.com/questions/42263270/what-is-the-concept-of-application-job-stage-and-task-in-spark</a></p></blockquote><h4><a name="解决的具体问题" class="md-header-anchor"></a><span>解决的具体问题</span></h4><p><code>Hadoop</code><span>不能基于内存共享数据，</span><mark><span>反复读写磁盘</span></mark><span>。因此，</span><code>Hadoop</code><span>的</span><code>MapReduce</code><span>对</span><mark><span>迭代</span></mark><span>式算法支持的效率不高，更别提图算法和机器学习算法了。</span></p><p><code>Spark</code><span>适应于在交互式数据挖掘工具中反复查询一个数据子集的应用场景。</span></p><h4><a name="缓存cache）" class="md-header-anchor"></a><span>缓存（Cache）</span></h4><p><span>将RDD保存到内存（也可能在磁盘上），读写速度极快。</span></p><p><span>实现方式是在</span><code>Persist</code><span>持久化时将</span><code>Storgelevel</code><span>设置为</span><code>MEMORY_ONLY</code></p><p><span>共12种</span><code>StorageLevel</code><span>；本质上和</span><code>Persist</code><span>没有区别，都是</span><code>Persist</code><span>方法，只是级别不同。</span></p><h5><a name="作用-n1419" class="md-header-anchor"></a><span>作用</span></h5><p><span>计算RDD完成后对其进行缓存，若整个计算失败，可以从缓存读取这个RDD，避免重新计算这个RDD，提升容错效率。</span></p><h5><a name="适用场景" class="md-header-anchor"></a><span>适用场景</span></h5><ul><li><span>获取大量数据后；</span></li><li><span>进行一个非常长的计算链条，设置一些缓存点；</span></li><li><span>某个步骤计算非常耗时，步骤完成后对结果进行缓存；</span></li><li><span>进行</span><code>checkpoint</code><span>前也会缓存；</span></li></ul><h4><a name="checkpoint" class="md-header-anchor"></a><span>Checkpoint</span></h4><p><span>将RDD持久化到HDFS，利用HDFS的容错能力降低RDD数据丢失的风险。</span></p><p><span>它会将RDD的依赖清空，如果HDFS也不能保证数据不丢失，则任务需要重新启动。</span></p><h4><a name="四个层次的容错" class="md-header-anchor"></a><span>四个层次的容错</span></h4><ul><li><p><span>Stage输出失败，上层调度器</span><code>DAGScheduler</code><span>重试</span></p></li><li><p><span>Task内部任务失败，底层</span><code>TaskScheduler</code><span>调度器重试</span></p></li><li><p><span>根据RDD Lineage血统重新计算</span></p><ul><li><span>对于宽依赖而言，如果结果的一个</span><code>Partition</code><span>出错，需要重新计算父</span><code>RDD</code><span>所有</span><code>Partition</code></li><li><span>对于窄依赖而言，如果结果的一个</span><code>Partition</code><span>出错，只需重新计算父RDD被丢失</span><code>Partition</code><span>依赖的那个</span><code>Partition</code></li><li><code>Cache</code><span>可看作时</span><code>Lineage</code><span>容错机制的效率提升机制，本身并无容错考虑，如果</span><code>Cache</code><span>丢失则仍需重新计算</span></li></ul></li><li><p><span>Checkpoint机制</span></p></li></ul><h3><a name="scheduler" class="md-header-anchor"></a><code>Scheduler</code></h3><p><span>调度器，简洁清晰和高效。</span></p><ul><li><span>输入：Spark RDD</span></li><li><span>输出：执行器Executor</span></li></ul><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018141659263.png" referrerpolicy="no-referrer" alt="image-20211018141659263"></p><h4><a name="spark-driver" class="md-header-anchor"></a><code>Spark Driver</code></h4><p><span>是运行</span><code>Application</code><span>的</span><code>main</code><span>函数。</span></p><p><span>负责初始化</span><code>SparkContext</code><span>，它负责。</span></p><p><span>与集群通讯、资源申请、任务分配和监控等。</span></p><h4><a name="流程" class="md-header-anchor"></a><span>流程</span></h4><h5><a name="初始化sparkcontext" class="md-header-anchor"></a><span>初始化</span><code>SparkContext</code></h5><p><span>初始化了</span><code>TaskScheduler</code><span>，</span><code>SchedulerBackend</code><span>，</span><code>DAGScheduler</code><span>。</span></p><h5><a name="rdd-transformation操作-rddtransformationlazy" class="md-header-anchor"></a><code>RDD Transformation</code><span>操作 </span><em><code>RDD.Transformation</code><span>==Lazy</span></em></h5><p><span>只记录RDD之间依赖关系，和操作类型，不具体调用</span><code>compute</code><span>方法。</span></p><h5><a name="触发点action操作-rddaction触发sparkcontext的runjob" class="md-header-anchor"></a><span>触发点：Action操作 </span><em><code>RDD.action</code><span>触发</span><code>SparkContext</code><span>的</span><code>runJob</code></em></h5><p><span>调用</span><code>runJob</code><span>，触发</span><code>DAGScheduler</code><span>调用</span><code>submitJob</code><span>。</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018142148221.png" referrerpolicy="no-referrer" alt="image-20211018142148221"></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018142232497.png" referrerpolicy="no-referrer" alt="image-20211018142232497"></p><h5><a name="dagscheduler构建stage" class="md-header-anchor"></a><code>DAGScheduler</code><span>构建</span><code>Stage</code></h5><p><span>在</span><code>createStage</code><span>时从最后一个</span><code>Stage</code><span>开始递归构建</span><code>Stage</code><span>，并根据</span><code>dependency</code><span>类型划分</span><code>Stage</code><span>。</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018142405321.png" referrerpolicy="no-referrer" alt="image-20211018142405321"></p><h5><a name="dagscheduler构建stage-n1475" class="md-header-anchor"></a><code>DAGScheduler</code><span>构建</span><code>Stage</code></h5><p><span>在提交</span><code>Stage</code><span>时从最后一个</span><code>Stage</code><span>开始回溯直到没有前序</span><code>Stage</code><span>的第一个</span><code>Stage</code><span>，执行</span><code>submitMissingTask</code><span>。</span></p><p><span>构建好</span><code>TaskSet</code><span>，计算每一个</span><code>Task</code><span>最佳位置，将</span><code>TaskSet</code><span>提交给</span><code>TaskScheduler</code><span>。</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018142641421.png" referrerpolicy="no-referrer" alt="image-20211018142641421"></p><h5><a name="taskscheduler针对每一个taskset细粒度调度和执行" class="md-header-anchor"></a><code>TaskScheduler</code><span>针对每一个</span><code>TaskSet</code><span>细粒度调度和执行</span></h5><ul><li><span>根据</span><code>Task</code><span>位置和执行器信息分配</span><code>Task</code><span>到</span><code>Executor</code><span>。</span></li><li><span>通过</span><code>SchedulerBackend</code><span>将执行命令发送到</span><code>Executor</code><span>上开始执行。</span></li><li><span>通过</span><code>SchedulerBackend</code><span>获得执行器相关信息。</span></li></ul><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018142836729.png" referrerpolicy="no-referrer" alt="image-20211018142836729"></p><blockquote><p><span>注意图中的两个调度算法。先来先服务和公平调度。</span></p><p><span>要先知道执行器的相关信息才能发送执行，因此要看Driver、Master、Work如何通信！</span></p></blockquote><h3><a name="executor" class="md-header-anchor"></a><code>Executor</code></h3><h4><a name="创建executor" class="md-header-anchor"></a><span>创建</span><code>Executor</code></h4><ul><li><p><span>首先</span><code>Application</code><span>要向</span><code>Master</code><span>注册——</span><mark><span>由</span><code>SchedulerBackend</code><span>通过</span><code>AppClient</code><span>的子类</span><code>ClientEndpoint</code><span>完成</span></mark><span>；</span></p></li><li><p><code>Master</code><span>回复注册成功，并将</span><code>Application</code><span>发送到满足条件的</span><code>Worker</code><span>上，在</span><code>Work</code><span>上启动</span><code>Executor</code><span>；</span></p></li><li><p><span>通过</span><code>Worker</code><span>的</span><code>RpcEndpoint</code><span>向</span><code>Worker</code><span>发送启动</span><code>Executor</code><span>请求；</span></p></li><li><p><code>Worker</code><span>收到请求后启动</span><code>ExecutorRunner</code><span>，并通知</span><code>Master Executor</code><span>的相关信息；</span></p></li><li><p><span>此时本质上是启动了负责帮</span><code>Executor</code><span>和</span><code>Driver</code><span>通信的</span><code>Backend</code><span>：</span><code>ExecutorBackend</code><span>；</span></p></li><li><p><code>ExecutorBackend</code><span>直接向Driver注册</span><code>Worker</code><span>让它准备启动的</span><code>Executor</code><span>；</span></p></li><li><p><span>收到</span><code>Driver</code><span>确认后，</span><code>Executor</code><span>正式新建；</span></p><p><mark><span>是</span><code>Executor</code><span>主动联系</span><code>Driver</code><span>，让</span><code>SchedulerBackend</code><span>了解到所有</span><code>Executor</code><span>的信息，再让</span><code>TaskScheduler</code><span>根据</span><code>Executor</code><span>情况分配</span><code>Task</code><span>到</span><code>Executor</code></mark></p></li></ul><h4><a name="executor资源分配" class="md-header-anchor"></a><code>Executor</code><span>资源分配</span></h4><ul><li><code>Driver</code><span>启动的时候从</span><code>spark-submit</code><span>收集对</span><code>executor</code><span>的需求，并在向</span><code>Master</code><span>注册时发送过去；</span></li><li><span>具体可以参考如何从提交程序收集需求，并一直传递给</span><code>Master</code><span>，再给</span><code>Worker</code><span>；</span></li><li><code>ExecutorRunner</code><span>从</span><code>Worker</code><span>那里得到这些参数，并在根据参数启动</span><code>ExecutorBackend</code><span> </span><code>JVM</code><span>进程；</span></li></ul><h4><a name="executor启动" class="md-header-anchor"></a><code>Executor</code><span>启动</span></h4><ul><li><span>在</span><code>ExecutorBackend</code><span>向</span><code>Driver</code><span>注册后，收到确认时的操作就是创建</span><code>Executor</code><span>；</span></li><li><span>此时</span><code>TaskScheduler</code><span>根据</span><code>Executor</code><span>情况安排好</span><code>TaskSet</code><span>调度，</span><code>Executor</code><span>也启动好了；</span></li></ul><h4><a name="通过driverendpoint向executorbackend发送执行task的消息" class="md-header-anchor"></a><span>通过</span><code>DriverEndpoint</code><span>向</span><code>ExecutorBackend</code><span>发送执行</span><code>Task</code><span>的消息</span></h4><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018144030533.png" referrerpolicy="no-referrer" alt="image-20211018144030533"></p><h4><a name="executorbackend收到launchtask通知后" class="md-header-anchor"></a><code>ExecutorBackend</code><span>收到</span><code>LaunchTask</code><span>通知后</span></h4><ul><li><p><span>调用</span><code>Executor</code><span>的</span><code>launchTask</code><span>方法；</span></p></li><li><p><span>内部创建</span><code>TaskRuner</code><span>，用线程池执行</span><code>TaskRunner</code><span>，调用</span><code>run</code><span>方法；</span></p></li><li><ul><li><span>反序列化出</span><code>Task</code></li><li><span>调用</span><code>Task</code><span>的</span><code>run</code><span>方法</span></li></ul></li></ul><h3><a name="storage" class="md-header-anchor"></a><code>Storage</code></h3><h4><a name="通信层" class="md-header-anchor"></a><span>通信层</span></h4><p><code>Master-Slave</code><span>结构，传输控制和状态信息。</span></p><p><code>BlockManager</code><span>, </span><code>BlockManagerMaster</code><span>, </span><code>BlockManagerMasterEndpoint</code><span>, </span><code>BlockManagerSlaveEndpoint</code><span>。</span></p><h4><a name="存储层" class="md-header-anchor"></a><span>存储层</span></h4><p><span>负责将数据存储到内存、磁盘或堆外内存中，为数据在远程节点生成副本。</span></p><p><code>DiskStore</code><span>, </span><code>MemoryStore</code><span>。</span></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018144317048.png" referrerpolicy="no-referrer" alt="image-20211018144317048"></p><p><img src="https://oss.ydjsir.com.cn/GitPages/SparkWithHadoop/image-20211018144453854.png" referrerpolicy="no-referrer" alt="image-20211018144453854"></p><h3><a name="shuffle" class="md-header-anchor"></a><code>Shuffle</code></h3><h4><a name="1rdd创建过程" class="md-header-anchor"></a><span>【1】RDD创建过程</span></h4><p><span>构建</span><code>RDD</code><span>时确定好与父RDD之间的依赖关系，如果是</span><code>ShuffleDependency</code><span>，会向</span><code>ShuffleManager</code><span>注册，获取</span><code>Handle</code><span>，用来保存父RDD的相关信息。</span></p><h4><a name="2rdd计算过程" class="md-header-anchor"></a><span>【2】</span><code>RDD</code><span>计算过程</span></h4><p><span>在实际计算时，</span><code>runTask</code><span>会调用</span><code>RDD compute</code><span>方法，其中根据</span><code>dependency</code><span>获取</span><code>reader</code><span>读取用来计算</span><code>RDD</code><span>的输入数据，也就是之前</span><code>shuffle</code><span>操作写入的数据。</span></p><h4><a name="3task结束处理" class="md-header-anchor"></a><span>【3】</span><code>Task</code><span>结束处理</span></h4><p><span>当</span><code>runTask</code><span>完成计算后，获取子</span><code>RDD</code><span>之间的依赖关系，如果是</span><code>shuffle</code><span>依赖则同样通过</span><code>dependency</code><span>和</span><code>ShuffleManager</code><span>获得</span><code>Handle</code><span>，进而获得</span><code>writer</code><span>。</span></p><h4><a name="可插拔的shuffle框架" class="md-header-anchor"></a><span>可插拔的</span><code>Shuffle</code><span>框架</span></h4><p><span>•</span><code>ShuffleDependency</code><span>，</span><code>ShuffleManager</code><span>，</span><code>ShuffleHandle</code></p><p><span>•</span><code>ShuffleReader</code><span>，</span><code>ShuffleWriter</code></p><p><span>•</span><code>ShuffleMapTask</code><span>，</span><code>ResultTask</code></p><h4><a name="拓展-n1557" class="md-header-anchor"></a><span>拓展</span></h4><p><code>Shuffle</code><span>前后必不可少地需要网络I/O，因此通过数据序列化方法和压缩技术进行效率优化。</span></p><blockquote><p><span>Spark中序列化方法和压缩算法的配置</span></p></blockquote><h5><a name="spark-10基于hash的shuffle机制" class="md-header-anchor"></a><code>Spark 1.0</code><span>：基于</span><code>Hash</code><span>的</span><code>Shuffle</code><span>机制</span></h5><p><span>每一个</span><code>Mapper</code><span>阶段的</span><code>Task</code><span>都会为</span><code>Reduce</code><span>阶段的每一个</span><code>Task</code><span>生成1个文件，</span><mark><span>M*R</span></mark><span>个。</span></p><p><span>合并机制：每一个执行单位为</span><code>Reduce</code><span>阶段每一个</span><code>Task</code><span>生成</span><mark><span>1个</span></mark><span>文件。</span></p><h5><a name="spark-11基于sort的shuffle机制" class="md-header-anchor"></a><code>Spark 1.1</code><span>：基于</span><code>Sort</code><span>的</span><code>Shuffle</code><span>机制</span></h5><p><span>每一个</span><code>Mapper</code><span>阶段的</span><code>Task</code><span>生成两个文件：索引和数据文件，</span><code>Reduce</code><span>阶段通过索引读取。</span></p><h5><a name="spark-14钨丝计划------优化内存管理模型" class="md-header-anchor"></a><code>Spark 1.4</code><span>：钨丝计划——优化内存管理模型</span></h5><p><span>直接使用</span><mark><span>二进制数据</span></mark><span>，而不是</span><code>Java</code><span>对象；避免序列化和反序列化开销。</span></p><h2><a name="部署实验" class="md-header-anchor"></a><span>部署实验</span></h2><p><span>详情参见 </span><a href='https://ydjsir.com.cn/2021/10/17/initSpark/' target='_blank' class='url'>https://ydjsir.com.cn/2021/10/17/initSpark/</a><span>。</span></p><h2><a name="实践" class="md-header-anchor"></a><span>&quot;实践&quot;</span></h2><blockquote><p><span>参考：</span><a href='https://spark.apache.org/docs/2.1.1/programming-guide.html' target='_blank' class='url'>https://spark.apache.org/docs/2.1.1/programming-guide.html</a></p></blockquote><h3><a name="基础" class="md-header-anchor"></a><span>基础</span></h3><h4><a name="从数据源到rdd" class="md-header-anchor"></a><span>从数据源到RDD</span></h4><ul><li><code>parallelize()</code><span> （把一个普通的collection变成支持分布式的）</span></li><li><code>textFile(path)</code></li><li><code>hadoopFile(path)</code></li><li><code>sequenceFile(path)</code></li><li><code>objectFile(path)</code></li><li><code>binaryFiles(path)</code></li><li><span>……</span></li></ul><h4><a name="从rdd到目标数据" class="md-header-anchor"></a><span>从RDD到目标数据</span></h4><p><span>•</span><code>saveAsTextFile(path)</code></p><p><span>•</span><code>saveAsSequenceFile(path)</code></p><p><span>•</span><code>saveAsObjectFile(path)</code></p><p><span>•</span><code>saveAsHadoopFile(path)</code></p><p><span>•……</span></p><h4><a name="二次排序" class="md-header-anchor"></a><span>二次排序</span></h4><ul><li><span>指在</span><mark><span>归约（reduce）阶段</span></mark><span>对</span><mark><span>某个键关联的值</span></mark><span>排序；</span></li><li><mark><span>Map阶段</span></mark><span>可以对</span><code>&lt;key, value&gt;</code><span>对按照键的值进行排序，但是归约器不会自动对键值对按照值排序；但是有时候需要：将成绩按照班级</span><mark><span>归约后排序</span></mark><span>；店铺产品销量排序等</span></li></ul><h5><a name="例按照name第一标准）和time第二标准）对value排序" class="md-header-anchor"></a><span>例：按照</span><code>name</code><span>（第一标准）和</span><code>time</code><span>（第二标准）对</span><code>value</code><span>排序</span></h5><p><img src="SparkWithHadoop.assets/image-20211021093117197.png" referrerpolicy="no-referrer" alt="image-20211021093117197"></p><p><span>归约只能做到按照</span><code>name</code><span>把值区分开，但是分好后再对</span><code>value</code><span>按照</span><code>time</code><span>排序时，需要进行二次排序。</span></p><h5><a name="方案1在内存中实现排序只借助map-reduce框架进行分组" class="md-header-anchor"></a><span>方案1：在内存中实现排序，只借助</span><code>Map-Reduce</code><span>框架进行分组</span></h5><p><span>组内直接在内存中调用排序函数。</span></p><ul><li><span>创建</span><code>SparkContext</code><span>对象；连接到</span><code>Spark master</code><span>；读取原始数据；</span></li><li><span>构建&lt;键,值&gt;对;</span></li><li><span>按照键分组：</span><code>groupByKey</code><span>；</span></li><li><span>对每个组对应的新的value(是一个列表)进行排序操作；</span></li></ul><p><span>不具备伸缩性，单个服务器的内存又成为瓶颈。</span></p><h5><a name="方案21利用框架实现值排序" class="md-header-anchor"></a><span>方案2.1：利用框架实现值排序</span></h5><p><span>自定义Key + </span><code>sortByKey()</code><span>：组合键。</span></p><ul><li><span>自定义组合键：</span><code>&lt;&lt;name, time&gt;, value&gt;</code><span>；</span></li><li><span>调用</span><code>sortByKey</code><span>时会用</span><code>&lt;name, time&gt;</code><span>排序，因此要比较</span><code>&lt;name, time&gt;</code><span>大小；</span></li><li><span>在自定义</span><code>Key</code><span>中定义好</span><code>compare</code><span>方法；</span></li><li><span>按照自定义Key格式实现</span><code>mapToPair</code></li><li><span>最后调用</span><code>sortByKey()</code></li></ul><h5><a name="方案22使用组合键--groupbykey" class="md-header-anchor"></a><span>方案2.2：使用组合键 + </span><code>groupByKey()</code></h5><p><img src="SparkWithHadoop.assets/image-20211021094415780.png" referrerpolicy="no-referrer" alt="image-20211021094415780"></p><h4><a name="top-n列表" class="md-header-anchor"></a><code>TOP N</code><span>列表</span></h4><h5><a name="基本思路" class="md-header-anchor"></a><span>基本思路</span></h5><p><span>在每一个</span><code>Partition</code><span>内取本地的</span><code>Top N</code><span>；将所有本地</span><code>Top N</code><span>合并，再取全局</span><code>Top N</code><span>。</span></p><h6><a name="使用mappartitions" class="md-header-anchor"></a><span>使用</span><code>mapPartitions()</code></h6><ul><li><span>对</span><code>RDD</code><span>的每一个</span><code>Partition</code><span>进行操作；</span></li><li><span>输入是整个</span><code>Partition</code><span>的数据，每一个</span><code>Task</code><span>对应处理一个</span><code>Partition</code><span>；</span></li><li><span>结果</span><code>RDD</code><span>与输入</span><code>RDD</code><span>有相同个数的</span><code>Partition</code><span>；</span></li><li><span>结果</span><code>RDD</code><span>的每一个</span><code>Partition</code><span>就是局部</span><code>Top N</code><span>；</span></li></ul><h6><a name="对保存所有局部top-n的rdd进行action操作" class="md-header-anchor"></a><span>对保存所有局部</span><code>Top N</code><span>的</span><code>RDD</code><span>进行</span><code>action</code><span>操作</span></h6><ul><li><span>用</span><code>collect</code><span>方法将局部</span><code>Top N</code><span>存放到</span><code>list</code><span>中</span></li><li><span>遍历</span><code>list</code><span>，选出全局</span><code>Top N</code></li></ul><h6><a name="使用框架的reduce操作" class="md-header-anchor"></a><span>使用框架的</span><code>reduce</code><span>操作</span></h6><p><span>定义两两合并的规则。</span></p><h5><a name="一种可行的解" class="md-header-anchor"></a><span>一种可行的解</span></h5><ul><li><span>使用框架的</span><code>takeOrdered(N, DefineComparator)</code></li><li><span>支持自定义</span><code>Comparator</code></li><li><span>得到</span><code>JavaPairRDD</code><span>后，直接使用</span><code>takeOrdered</code><span>获得全局</span><code>Top N</code></li></ul><h3><a name="spark-sql-n1840" class="md-header-anchor"></a><code>Spark SQL</code></h3><p><span>Spark SQL是</span><code>Spark Core</code><span>上的一个模块，所有</span><code>SQL</code><span>操作被</span><code>Catalyst</code><span>翻译成类似普通</span><code>Spark</code><span>程序一样的代码，被</span><code>Spark Core</code><span>调度执行，过程也有</span><code>Job</code><span>、</span><code>Stage</code><span>和</span><code>Task</code><span>概念。</span></p><p><span>它是根据待处理数据和待执行计算的结构信息，做了额外优化。</span></p><h4><a name="spark-catalyst" class="md-header-anchor"></a><span>Spark Catalyst</span></h4><ul><li><span>解析、优化</span><code>Spark SQL</code><span>语句，最终生成</span><code>Java</code><span>字节码；</span></li><li><span>使用核心数据结构-树-存储</span><code>SQL</code><span>语句；</span></li><li><span>使用基础概念-规则-对</span><code>SQL</code><span>语句对应的计算进行优化；</span></li></ul><p><img src="SparkWithHadoop.assets/image-20211021100418147.png" referrerpolicy="no-referrer" alt="image-20211021100418147"></p><figure><table><thead><tr><th><span>实际用例</span></th><th><span>Catalyst</span></th></tr></thead><tbody><tr><td><img src="SparkWithHadoop.assets/image-20211021100100958.png" referrerpolicy="no-referrer" alt="image-20211021100100958"></td><td><img src="SparkWithHadoop.assets/image-20211021100111405.png" referrerpolicy="no-referrer" alt="image-20211021100111405"></td></tr></tbody></table></figure><h4><a name="使用途径" class="md-header-anchor"></a><span>使用途径</span></h4><h5><a name="使用sql语句进行数据查询" class="md-header-anchor"></a><span>使用</span><code>SQL</code><span>语句进行数据查询</span></h5><ul><li><code>Spark SQL</code><span>可以读取</span><code>Hive</code><span>上的数据；</span></li><li><span>可以在程序中执行</span><code>SQL</code><span>查询，结果以</span><code>dataset/dataframe</code><span>形式返回；</span></li><li><span>通过命令行使用</span><code>SQL</code><span>命令；</span></li></ul><h5><a name="在程序中使用dataset-api" class="md-header-anchor"></a><span>在程序中使用</span><code>Dataset API</code></h5><ul><li><code>Dataset</code><span>也是分布式数据集合，具有与</span><code>RDD</code><span>一样的优点，还根据</span><code>Spark SQL</code><span>引擎的特性进行了优化；</span></li><li><span>可以从</span><code>JVM</code><span>中的对象构建并使用类似</span><code>RDD</code><span>可用的</span><code>Transformation</code><span>操作；</span></li></ul><h5><a name="dataframe也是一种dataset数据有列的概念类似关系型数据库的表概念" class="md-header-anchor"></a><code>Dataframe</code><span>：也是一种</span><code>Dataset</code><span>，数据有列的概念，类似关系型数据库的“表”概念。</span></h5><ul><li><span>比</span><code>Dataset</code><span>更加丰富的操作；</span></li><li><span>多种来源：结构化数据文件，</span><code>Hive表</code><span>，外部数据库，</span><code>RDD</code><span>；</span></li></ul><h3><a name="spark-streaming-n1930" class="md-header-anchor"></a><code>Spark Streaming</code></h3><blockquote><p><span>这几张图检索一下就会发现来自于Spark官方文档</span></p></blockquote><h4><a name="structured-streaming" class="md-header-anchor"></a><code>Structured Streaming</code></h4><p><code>Structured Streaming</code><span>基于</span><code>Spark SQL engine</code><span>。 </span></p><p><span>在动态变化的数据集上实现流式计算就像在静态数据集上做计算一样方便。</span></p><p><img src="SparkWithHadoop.assets/image-20211021101018873.png" referrerpolicy="no-referrer" alt="image-20211021101018873"></p><h4><a name="对dataframesdatasets进行多种操作" class="md-header-anchor"></a><span>对</span><code>DataFrames/DataSets</code><span>进行多种操作</span></h4><h5><a name="基本操作" class="md-header-anchor"></a><span>基本操作</span></h5><ul><li><code>select</code><span>, </span><code>projection</code><span>, </span><code>aggregation</code><span>, </span><code>groupBy</code><span>, </span><code>groupByKey</code><span>, </span><code>filter</code><span>……</span></li><li><span>创建数据表，并使用SQL操作</span></li><li><span>判断是否是流</span></li></ul><h5><a name="window-operations-on-event-time" class="md-header-anchor"></a><span>Window Operations on Event Time</span></h5><p><img src="SparkWithHadoop.assets/image-20211021101455071.png" referrerpolicy="no-referrer" alt="image-20211021101455071"></p><p><span>5分钟统计一次，10分钟是一个窗口。统计的不仅仅是从开始时间点到数据截止时的总出现次数，而是这10分钟的窗口期内出现的次数。</span></p><h5><a name="handling-late-data-and-watermarking" class="md-header-anchor"></a><span>Handling Late Data and Watermarking</span></h5><p><img src="SparkWithHadoop.assets/image-20211021101806557.png" referrerpolicy="no-referrer" alt="image-20211021101806557"></p><p><span>迟一点不要紧，一样可以更新在表格里面。但是等待不是没有限度的，在那个</span><code>watermark</code><span>线以下的就直接</span><code>ignore</code><span>了。</span></p><p><img src="SparkWithHadoop.assets/image-20211021101912017.png" referrerpolicy="no-referrer" alt="image-20211021101912017"></p><h6><a name="启动计算" class="md-header-anchor"></a><span>启动计算</span></h6><p><span>当定义好最后一个</span><code>DataFrames</code><span>或</span><code>DataSets</code><span>，就剩下启动流计算了。</span></p><p><strong><span>输出的细节</span></strong><span>: 数据格式和位置等.</span></p><p><strong><span>输出模式</span></strong><span>：每一次计算之后哪些数据被写入，包括Append模式，Complete模式和Update模式.</span></p><p><strong><span>应用名称</span></strong><span>: 可选的，为该结构化流计算命名，是唯一的.</span></p><p><strong><span>触发时间间隔</span></strong><span>: 可选的，定义每一次计算的触发时间间隔. 如果不设置，则在上一次计算结束后立即启动. 如果上一次计算太久导致错过设置的触发时间，系统不会等待下一个时间间隔，而是上一个任务结束就启动计算。</span></p><p><strong><span>检查点存储位置</span></strong><span>: 应该是一个和HDFS兼容的高容错文件系统目录。</span></p><p><strong><span>内置的可写入输出</span></strong><span>：</span><code>File sink</code><span>, </span><code>Kafka sink</code><span>, </span><code>Foreach sink</code><span>, </span><code>Console sink</code><span>, </span><code>Memory sink</code><span>。 </span></p><h4><a name="streaming" class="md-header-anchor"></a><code>Streaming</code></h4><p><span>实现可扩展、高吞吐、高容错的实时数据流处理。</span></p><p><img src="SparkWithHadoop.assets/image-20211021102348551.png" referrerpolicy="no-referrer" alt="image-20211021102348551"></p><h5><a name="部署条件" class="md-header-anchor"></a><span>部署条件</span></h5><p><span>集群要求，JAR要求，配置要求，检查点配置，重启配置，日志配置，流配置等等。</span></p><ul><li><span>执行监控</span></li><li><span>性能优化</span></li><li><span>降低数据处理时间（数据接收并行化、数据处理并行化、数据序列化、任务数量控制）</span></li><li><span>巧妙设置时间间隔</span></li><li><span>内存优化</span></li><li><span>容错</span></li></ul><p><span>如何对已经部署的应用进行更新？</span></p><ul><li><span>数据写入多个地方，同时启动更新应用；</span></li><li><span>停止旧应用再启动新应用；</span></li></ul><h5><a name="需要注意的地方" class="md-header-anchor"></a><span>需要注意的地方</span></h5><ul><li><span>一旦一个流计算过程在</span><code>Context</code><span>中启动后，这个</span><code>Context</code><span>不可以再新建新的流计算过程；</span></li><li><span>一旦</span><code>Context</code><span>停止后，不可以被重新启动——只能重新提交</span><code>Application</code><span>；</span></li><li><span>一个</span><code>JVM</code><span>一次只能运行一个</span><code>StreamingContext</code><span>；</span></li><li><code>StreamingContext</code><span>的</span><code>stop</code><span>方法也会停止</span><code>SparkContext</code><span>，除非指定不停止</span><code>SparkContext</code><span>；</span></li><li><code>SparkContext</code><span>如果不停止，可以用于重复建立新的</span><code>StreamingContext</code><span>；</span></li></ul><h3><a name="spark-graphx-n2005" class="md-header-anchor"></a><code>Spark GraphX</code></h3><p><span>图计算：以图为数据结构基础的相关算法及应用。</span></p><h4><a name="graphx提供的api" class="md-header-anchor"></a><code>GraphX</code><span>提供的</span><code>API</code></h4><ul><li><span>图生成。</span></li><li><span>图数据访问：查询顶点数、边数；计算某个点的入度、出度等。</span></li><li><span>图算法：遍历顶点、边；计算连通性；计算最大子图；计算最短路径；图合并等。</span></li></ul><h4><a name="graphx的实现" class="md-header-anchor"></a><code>GraphX</code><span>的实现</span></h4><ul><li><span>核心是</span><code>Graph</code><span>数据结构，表示有向多重图；</span></li><li><span>两个顶点间允许存在多条边，表示不同含义；</span></li><li><code>Graph</code><span>由顶点</span><code>RDD</code><span>和边</span><code>RDD</code><span>组成；</span></li><li><code>Graph</code><span>的分布式存储方式；</span></li></ul><h4><a name="细节" class="md-header-anchor"></a><span>细节</span></h4><h5><a name="图生成" class="md-header-anchor"></a><span>图生成</span></h5><ul><li><span>读入存储关系信息的文件，构造</span><code>EdgeRDD：eRDD = sc.textFile()</code><span>；</span></li><li><span>从</span><code>Edge RDD</code><span>构造</span><code>Graph：graph = Grapth.fromEdges(eRDD)</code><span>；</span></li></ul><h5><a name="基本接口" class="md-header-anchor"></a><span>基本接口</span></h5><ul><li><span>获取边数：</span><code>numEdges</code><span>；获取节点数：</span><code>numVertices</code><span>；</span></li><li><span>获取入度、出度：</span><code>inDegrees</code><span>, </span><code>outDegrees</code><span>；</span></li><li><span>结构操作：</span><code>reverse</code><span>，</span><code>subgraph</code><span>, </span><code>mask</code><span> ；</span></li></ul><h5><a name="关联类操作" class="md-header-anchor"></a><span>关联类操作</span></h5><ul><li><span>将一个图和一个</span><code>RDD</code><span>通过顶点</span><code>ID</code><span>关联起来，使图获得</span><code>RDD</code><span>信息；</span></li><li><code>joinVertices</code><span>；</span></li><li><code>outerJoinVertices</code><span>； </span></li></ul><h5><a name="聚合类操作" class="md-header-anchor"></a><span>聚合类操作</span></h5><ul><li><span>分布式遍历所有的边，执行自定义的</span><code>sendMsg</code><span>函数；</span></li><li><span>在节点上执行</span><code>mergeMsg</code><span>函数；</span></li></ul><h3><a name="spark-mllib-n2009" class="md-header-anchor"></a><code>Spark MLlib</code></h3><p><span>Spark为机器学习问题开发的库。支持分类、回归、聚类和协同过滤等。</span></p><h4><a name="基础数据类型" class="md-header-anchor"></a><span>基础数据类型</span></h4><ul><li><span>向量</span></li><li><span>带标注的向量：用于监督学习</span></li><li><span>模型：训练算法的输出</span></li></ul><h4><a name="主要的库" class="md-header-anchor"></a><span>主要的库</span></h4><ul><li><code>mllib.classification</code><span>:分类算法，二分类、多分类、逻辑回归、朴素贝叶斯、SVM等；</span></li><li><code>mllib.cluster</code><span>：聚类，</span><code>K-Means</code><span>、</span><code>LDA</code><span>等；</span></li><li><code>mllib.recommendation</code><span>：使用协同过滤的方法做推荐；</span></li><li><code>mllib.tree</code><span>：决策树、随机森林等算法；</span></li></ul><h4><a name="其它常用库" class="md-header-anchor"></a><span>其它常用库</span></h4><p><code>mllib.evaluation</code><span>：算法效果衡量方法。</span></p></div>
</body>
</html>